---
title: 'HarvardX: PH125.9x Data Science: Capstone - MovieLens Project'
author: "Martin Weihrauch"
date: "January 19, 2019"
output: pdf_document
---


# 1. Executive Summary
## Introduction
In 2006, Netflix offered a million US-Dollar price to whomever was able to surpass the performance of their movie recommendation algorithm by at least 10%. This goal was achieved by ["Team BellKor's Pragmatic Chaos""](https://www.netflixprize.com/community/topic_1537.html) in 2009, after a nearly 3-year long contest.
Recommendation systems are a common occurrence nowadays and companies, big and small, employ them to increase sales numbers. It doesn't matter whether it is a new book on Amazon you might like to read, or the next popular song you might want to listen to on Spotify.


## Aim of the project
The aim of this project is to build a machine learning algorithm that predicts user ratings for movies based on the dataset ["MovieLens"](http://files.grouplens.org/datasets/movielens/ml-10m.zip) with 10 million ratings. The algorithm is to be trained on a provided training subset (`edx`) of "MovieLens" and evaluation is to be carried out on the provided testing subset (`validation`).
The metric used to evaluate algorithm performance is the residual mean squared error, or RMSE.


## Dataset Preparation
The "MovieLens" dataset is automatically downloaded and prepared for analysis by splitting it into a training (90% of the data) and a test set (10% of the data. All necessary R packages are downloaded and loaded to execute the analysis.

```{r, echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}
#############################################################
# Create edx set, validation set, and submission file
#############################################################

# Note: this process could take a couple of minutes
# The vtreat package will be used for cross-validation purposes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(vtreat)) install.packages("vtreat", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```
\pagebreak

# 2. Methods and Analysis

## Exploratory Data Analysis
As a first step, we want to get familiar with our dataset. A quick glance at the first few entries reveals that each row represents a single rating of a user for a single movie, including the movie title, genre and timestamp at which the rating was given.


```{r, echo = FALSE}
head(edx) 
```


A quick summary of the dataset tells us that there are no missing values to deal with.



```{r, echo = FALSE}
summary(edx) 
```


We are dealing with over 9 million movie ratings of ~70000 unique users giving ratings to ~ 10700 different movies.



```{r, echo = FALSE}
edx %>% summarize(n_users = n_distinct(userId), n_movies = n_distinct(movieId))
```


We take a look at the distribution of ratings: 4 is the most common rating, followed by 3 and 5. 0.5 is the least common rating.



```{r, echo = FALSE}
edx %>%
  count(rating) %>%
  arrange(desc(n))
```

To further strengthen our intuition about the rating distribution, we take a look at a histrogram of it.


```{r, echo = FALSE, fig.height=4, fig.width=5}
edx %>%
  ggplot(aes(rating)) +
  geom_histogram(binwidth = 0.5, color = "black", fill = "orange") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  scale_y_continuous(breaks = c(seq(0, 3000000, 500000))) +
  ggtitle("Rating distribution") +
  theme_light()
```





```{r echo = FALSE, fig.height=4, fig.width=5}
edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black", fill = "orange") + 
  scale_x_log10() + 
  xlab("Number of ratings [log10]") +
  ylab("Number of movies") +
  ggtitle("Number of ratings per movie") +
  theme_light()
```

Some users were very active and rated a vast amount of movies, while others only rated a few.


```{r, echo = FALSE, fig.height=4, fig.width=5}
edx %>% count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black", fill = "orange") + 
  scale_x_log10() + 
  xlab("Number of ratings [log10]") +
  ylab("Number of users") +
  ggtitle("Number of ratings given by users") +
  theme_light()
```

Furthermore, users differ vastly in how many movies they rated and how critical they are with their ratings. Some users tend to give much lower star ratings and some users tend to give higher star ratings than average. The visualization below includes only users that have rated at least 100 movies.

```{r, echo = FALSE, fig.height=4, fig.width=5}
edx %>% 
  group_by(userId) %>% 
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating)) %>% 
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black", fill = "orange") +
  xlab("Mean rating") +
  ylab("Number of users") +
  ggtitle("Mean movie ratings given by users") +
  theme_light()
```



\pagebreak

## Modelling Approach

We write a loss-function that computes the Residual Mean Squared Error (RMSE, or "typical error") as
our measure of accuracy. The value is the typical error in star rating we would make upon predicting
a movie rating.

```{r, echo = TRUE}
RMSE <- function(predicted_ratings, true_ratings){
  sqrt(mean((predicted_ratings - true_ratings)^2))
}
```

We begin by building a very simple model: We predict a new movie rating to be the average rating of all
movies in our training dataset `edx`.
This gives us our baseline RMSE to compare future modelling approaches against.
We observe that the mean movie rating is a pretty generous > 3.5 stars, quite a bit above the "average" (as in 2.5 stars out of 5).
We create a table to record our approaches and the RMSEs they generate.


```{r, echo = TRUE}
mu <- mean(edx$rating)
print(mu)
baseline_RMSE <- RMSE(mu, edx$rating)
baseline_RMSE


rmse_results <- data_frame(Method = "Simplest model: Average rating",
                           RMSE = baseline_RMSE)
rmse_results
```


To improve upon the model, we utilize the fact that different movies are rated differently.
We compute the deviation of each movies' mean rating from the total mean of all movies `mu`.
We call the resulting variable "b" (as in "bias") for each movie "i": `b_i`



```{r, echo = TRUE}
movie_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))
```


When plotting our computed `b_i`, we can see that there are large differences in how different movies are rated.


```{r, echo = FALSE, fig.height=4, fig.width=5}
movie_avgs %>% qplot(b_i, geom = "histogram", bins = 10, data = ., color = I("black"), fill = I("orange"))
```


We then predict movie ratings based on the fact that different movies are rated differently by adding the computed `b_i` to `mu`. If an individual movie is on average rated worse than the average rating of all movies `mu`, we predict that it will be rated lower than `mu` by `b_i`, the difference of the individual movie average from the total average.


```{r, echo = TRUE}
predicted_ratings <- validation %>% 
  left_join(movie_avgs, by = "movieId") %>%
  mutate(pred = mu + b_i) %>%
  .$pred

model_1_RMSE <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results, data_frame(method = "Movie Effect Model", 
                                                   RMSE = model_1_RMSE))
rmse_results
```

We can see that the RMSE improves when we take into account that different movies are rated differently.
Do users rate different movies differently? We compute `b_u`, the user-specific effect.
We can see that some users rate movies generally higher/lower than others, while most fall in-between, but also that user rating of movies is generally higher than a 2.5 "true average" rating. This was reflected previously in the high mean rating of > 3.5. We then predict the ratings taking into account movie and user effects together.

```{r, echo = TRUE}
user_avgs <- edx %>%
  left_join(movie_avgs, by = "movieId") %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- validation %>%
  left_join(movie_avgs, by = "movieId") %>%
  left_join(user_avgs, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

model_2_RMSE <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results, data_frame(method = "Combined Movie & User Effects Model", 
                                                   RMSE = model_2_RMSE))
rmse_results
```


We can see that including the user-effect `b_u` in our rating predictions further reduced the RMSE.
It appears that we are still off by ~0.865 stars on average. Are we correctly predicting the best and worst movies with our model? The top ten best and worst movies according to our predictions so far appear to include some "obscure" entries. This might be due to an overall low amount of ratings associated with these movies. We take a look at the amount of ratings given to them.

We look at the top ten best movies according to our predictions so far (largest positive `b_i`), as well as at the number of ratings they received.
```{r, echo = FALSE}
movie_titles <- edx %>% 
  select(movieId, title) %>%
  distinct()

# Top 10 best movies according to our prediction with the largest positive b_i
# (movies rated better than average).
edx %>% count(movieId) %>% 
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(movie_titles, by = "movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```


The top ten worst movies according to our predictions so far (largest negative `b_i`).
```{r, echo = FALSE}
edx %>% count(movieId) %>% 
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(movie_titles, by = "movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

Indeed, some of the best and worst movies we predict were rated sparsly. Larger estimates of `b_i` are likely for movies with very few ratings. The same holds true for the user-effect `b_u`, in those cases where users only rated a very small number of movies. 
We can penalize these by making use of regularization.
We determine the Lambda that minimizes RMSE by 5-way cross validation. This shrinks the `b_i` and `b_u` in case of small number of ratings. Essentially, by shrinking our estimates when we are rather unsure, we are being more conservative in our estimations.

```{r, echo = FALSE, results = "hide"}
set.seed(1)
splitPlan <- kWayCrossValidation(nRows = nrow(edx), nSplits = 5, NULL, NULL) # We split our training data into k = 5 different train/test sets for cross-validation

lambdas <- seq(1.5, 3, 0.25) # We define the range of values we test for Lambda. The range has been set as small as possible to reduce computation times.
opt_lambda <- 0 # We initialize an empty vector that takes the results of the for-loop below

# NOTE: This code likely runs for several minutes, please be patient.
# The range for Lambda has been narrowed down after testing for all lambdas between 0 and 20 to shorten computation time here.

for (i in 1:length(splitPlan)){
  
split <- splitPlan[[i]]

rmses <- sapply(lambdas, function(lambda){
  
b_i <- edx[split$train, ] %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n() + lambda), n_i = n()) # Lambda is used to penalize b_i for small n()

# We generate a temporary test-set out of our training data edx
# by using the split from kWayCrossValidation. No information from our true validation set is used.
test_temp <- edx[split$app, ] %>%
  semi_join(edx[split$train, ], by = "movieId") %>%
  semi_join(edx[split$train, ], by = "userId")

# Prediction of ratings with temporary test-set
predicted_ratings <- test_temp %>%
  left_join(b_i, by = "movieId") %>%
  mutate(pred = mu + b_i) %>%
  .$pred

# Calculation of the resulting RMSE with Lambda
return(RMSE(predicted_ratings, test_temp$rating))
})

opt_lambda[i] <- lambdas[which.min(rmses)]

}
opt_lambda # All Lambda values from the k = 5 cross validations
b_i_opt_lambda <- mean(opt_lambda) # We use the mean as our optimal Lambda value
```


We then calculate the regularized movie-effect `b_i` using the optimised Lambda value. We also apply the same approach to the user-effect `b_u`.


```{r, echo = TRUE}
movie_reg_avgs <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - mu)/(n() + b_i_opt_lambda), n_i = n()) 
```

```{r, echo = FALSE, results = "hide"}
###
# Regularization for the user effect b_u.
###


lambdas <- seq(4.5, 5.5, 0.25)
opt_lambda <- 0 # Empty vector that takes the output of the for-loop below


# NOTE: This code likely runs for several minutes, please be patient.
# The range for Lambda has been narrowed down after testing for all lambdas
# between 0 and 20 to shorten computation time here.
for (i in 1:length(splitPlan)){
  
split <- splitPlan[[i]]

rmses <- sapply(lambdas, function(lambda){
    
b_u <- edx[split$train, ] %>% 
      left_join(movie_reg_avgs, by = "movieId") %>%
      group_by(userId) %>%
      summarize(b_u = sum(rating - b_i - mu)/(n() + lambda), n_i = n())
    
test_temp <- edx[split$app, ] %>%
      semi_join(edx[split$train, ], by = "movieId") %>%
      semi_join(edx[split$train, ], by = "userId")
    
predicted_ratings <- test_temp %>%
      left_join(movie_reg_avgs, by = "movieId") %>%
      left_join(b_u, by = "userId") %>%
      mutate(pred = mu + b_i + b_u) %>%
      .$pred

# Calculation of the resulting RMSE with Lambda
return(RMSE(predicted_ratings, test_temp$rating))
})
  
opt_lambda[i] <- lambdas[which.min(rmses)]
  
}
opt_lambda
b_u_opt_lambda <- mean(opt_lambda)
```

```{r, echo = TRUE}
user_reg_avgs <- edx %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  group_by(userId) %>% 
  summarize(b_u = sum(rating - b_i - mu)/(n() + b_u_opt_lambda), n_i = n()) 
```

We then predict the ratings with the regularized movie- and user-effects.

```{r, echo = FALSE}
predicted_ratings <- validation %>%
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(user_reg_avgs, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

model_3_RMSE <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method = "Combined, regularized Movie & User Effect",
                                     RMSE = model_3_RMSE))
rmse_results %>% knitr::kable()
```

Regularization had little impact on the RMSE, but did it improve the best and worst movies we predict?
We take a look at the 10 best movies after regularization.

```{r, echo = FALSE}
edx %>%
  count(movieId) %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(movie_titles, by = "movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

And at the 10 worst movies after regularization.

```{r, echo = FALSE}
edx %>%
  count(movieId) %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(movie_titles, by = "movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

Indeed, the movies we predict to be the best/worst make much more sense now.
\pagebreak

# Results

## Accuracy of the Model

While the predicted ratings have the same mean as the original ratings, we observe values below 0.5 and above 5 due to the way we calculated them.
Furthermore, we predicted numeric values and not categorical ratings from 0.5 to 5.
```{r, echo = TRUE}
summary(predicted_ratings)


head(predicted_ratings)
```


In order to predict star ratings from 0.5 to 5, we have to round our predictions and substitute values above 5 and below 0.5 accordingly.


```{r, echo = FALSE}
my_prediction <- predicted_ratings
my_prediction <- round(my_prediction/0.5)*0.5
my_prediction[my_prediction <= 0.5] <- 0.5 # Substitute all values below 0.5 with 0.5
my_prediction[my_prediction >= 5] <- 5 # Substitute all values above 5 with 5
```

The accuracy of our predictions in star ratings from 0.5 to 5.
```{r, echo = TRUE}
mean(my_prediction == validation$rating)
```



Our final RMSE value of the predicted ratings without rounding.

```{r, echo = TRUE}
RMSE(predicted_ratings, validation$rating)

rmse_results %>% knitr::kable()                              
```


We arrive at an accuracy of 24.8% and an RMSE of 0.8648490.
\pagebreak

# Conclusion

The ~25% prediction accuracy value of this model can be explained by several facts. User preference for certain types of movies, genres, or even particular actors and directors most likely plays a major role in determining the ratings given. In fact, many modelling approaches in the Netflix challenge incorporated more user information than what was given here and it was found that user profession has a large predictive value. Other possible approaches would include item- and/or user-based collaborative filtering, where the relatively sparse matrix of user ratings (not every user rated every movie in the dataset, in fact far from it) is filled by matching similar user's preferences.