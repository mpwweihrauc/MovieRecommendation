---
title: 'HarvardX: PH125.9x Data Science: Capstone - MovieLens Project'
author: "Martin Weihrauch"
date: "January 22, 2019"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---


# Executive Summary
This report begins by introducing the general idea and problem of the project and by stating its aims. After dataset preparation and setup, exploratory data analysis is carried out to gain the necessary insights into the data in order to develop a machine learning algorithm that predicts movie ratings. Next, the general modelling approach is broken down step-by-step and ends with the final model and its results in terms of prediction accuracy. This is then followed by a discussion section where strengths and weaknesses of the generated modelling approach are broken down and alternative approaches are briefly discussed. Finally, the report ends with some concluding remarks.

## Introduction
In 2006, Netflix offered a million US-Dollar price to whomever was able to surpass the performance of their movie recommendation algorithm by at least 10%. This goal was achieved by ["Team BellKor's Pragmatic Chaos""](https://www.netflixprize.com/community/topic_1537.html) in 2009, after a nearly 3-year long contest.
  Recommendation systems are a common occurrence nowadays and companies, big and small, employ them to increase sales numbers. It doesn't matter whether it is a new book on Amazon you might like to read, or the next popular song you might want to listen to on Spotify.
Movie recommendation systems work by knowing what a user likes (e.g. movie genres, actors) to then recommend more similar movies.
  It can become extremely challenging to recommend a movie to a user who really loves war dramas such as "Saving Private Ryan", but at the same time also really likes pacifist movies (or really hates one or a few particular war dramas, but loves all the rest). The more a recommendation system knows about what users like, the better a recommendation it can make.
  It is not the purpose of this project to give a "Top 10" best movies to watch recommendation list for a particular user, but instead to make predictions about the star rating users would give to different movies.


## Aim of the project
The aim of this project is to build a machine learning algorithm that predicts user ratings (from 0.5 to 5 stars) for movies based on the dataset ["MovieLens"](http://files.grouplens.org/datasets/movielens/ml-10m.zip) with 10 million user ratings of over 10500 different movies.
  The algorithm is to be trained on a provided training subset (`edx`) of ["MovieLens"](http://files.grouplens.org/datasets/movielens/ml-10m.zip) and evaluation is to be carried out on the provided testing subset (`validation`).
  The metric used to evaluate algorithm performance is the residual mean squared error, or RMSE. This represents the typical error in star ratings the predicted rating would be off by.


## Dataset Preparation
The "MovieLens" dataset is automatically downloaded and prepared for analysis by splitting it into a training (90% of the data) and a test set (10% of the data). All necessary R packages are downloaded and loaded to execute the analysis.

```{r, echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}
#############################################################
# Create edx set, validation set, and submission file
#############################################################

# Note: this process could take a couple of minutes
# The vtreat package will be used for cross-validation purposes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(vtreat)) install.packages("vtreat", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```
\pagebreak

# Methods and Analysis

## Exploratory Data Analysis
As a first step, we want to get familiar with our dataset. A quick glance at the first few entries reveals that each row represents a single rating of a user for a single movie, including the movie title, genre and timestamp at which the rating was given.


```{r, echo = FALSE}
head(edx) %>%
  print.data.frame()
```


A quick summary of the dataset tells us that there are no missing values to deal with.



```{r, echo = FALSE}
summary(edx)
```


We are dealing with over 9 million movie ratings of ~70000 unique users giving ratings to ~ 10700 different movies.



```{r, echo = FALSE}
edx %>%
  summarize(n_users = n_distinct(userId), n_movies = n_distinct(movieId)) %>%
  print.data.frame()
```


We take a look at the distribution of ratings: 4 is the most common rating, followed by 3 and 5. 0.5 is the least common rating. It becomes apparent that half-star ratings are much less common than full-star ratings.



```{r, echo = FALSE}
edx %>%
  count(rating) %>%
  arrange(desc(n)) %>%
  print.data.frame()
```

To further strengthen our intuition about the rating distribution, we take a look at a histrogram of it.


```{r, echo = FALSE, fig.height=4, fig.width=5}
edx %>%
  ggplot(aes(rating)) +
  geom_histogram(binwidth = 0.5, color = "black", fill = "orange") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  scale_y_continuous(breaks = c(seq(0, 3000000, 500000))) +
  ggtitle("Rating distribution") +
  theme_light()
```


We take a look at the number of times different movies have been rated. We can observe that some movies have been rated much more often than others, while some have gotten very few and sometimes only a single rating. This will be important to consider for our modelling approach, as very low rating numbers might give untrustworthy estimates for predictions.


```{r echo = FALSE, fig.height=4, fig.width=5}
edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black", fill = "orange") + 
  scale_x_log10() + 
  xlab("Number of ratings [log10]") +
  ylab("Number of movies") +
  ggtitle("Number of ratings per movie") +
  theme_light()
```

What are some of these movies with very low ratings? We take a look at 20 movies that only received a single rating. As expected, these movies appear to be rather obscure, and predictions of future ratings for these will be difficult.

```{r echo = FALSE, fig.height=4, fig.width=5}
edx %>% 
  group_by(movieId) %>%
  summarize(count = n()) %>% 
  filter(count == 1) %>%
  left_join(edx, by = "movieId") %>%
  group_by(title) %>%
  summarize(rating = rating, n_rating = count) %>%
  slice(1:20) %>%
  knitr::kable()
```



Next we will explore user rating behaviour. Some users were very active and rated a large amount of movies, while others only rated a few.


```{r, echo = FALSE, fig.height=4, fig.width=5}
edx %>% count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black", fill = "orange") + 
  scale_x_log10() + 
  xlab("Number of ratings [log10]") +
  ylab("Number of users") +
  ggtitle("Number of ratings given by users") +
  theme_light()
```

Furthermore, users differ vastly in how critical they are with their ratings. Some users tend to give much lower star ratings and some users tend to give higher star ratings than average. The visualization below includes only users that have rated at least 100 movies.

```{r, echo = FALSE, fig.height=4, fig.width=5}
edx %>% 
  group_by(userId) %>% 
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating)) %>% 
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black", fill = "orange") +
  xlab("Mean rating") +
  ylab("Number of users") +
  ggtitle("Mean movie ratings given by users") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  theme_light()
```

We take a look at the representation of movie genres, their average ratings and the difference of their average rating from $\mu$ ("norm_avg_rating"). We can see that dramas and comedies are quite overrepresented. Some genres clearly get rated better or worse than others, for example Film-Noir (on average getting a 4-star rating), war movies, documentaries and IMAX movies are highly appraised, while horror movies are frowned upon. However, modelling these differences between genres is potentially difficult, as most movies tend to have more than just one genre associated with them and certain effects might cancel eachother out, or worse, mislead the prediction. While the Film-Noir genre has a mean rating of 4, there are still many movies getting as low as 0.5 stars as well.

```{r, echo = FALSE}
mu <- mean(edx$rating)

edx %>%
  separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  summarize(count = n(), avg_rating = mean(rating), norm_avg_rating = mean(rating) - mu) %>%
  arrange(desc(count)) %>%
  print.data.frame()
```



\pagebreak

## Modelling Approach

We begin by writing a loss-function that computes the Residual Mean Squared Error (RMSE, or "typical error") as our measure of model accuracy. The value is the typical error in star rating we would make upon predicting a movie rating. $y_{u,i}$ here represents the rating for movie $i$ given by user $u$ and $N$ stands for the number of user/movie combinations. We denote our rating predictions from user $u$ for movie $i$ as $\hat{y}_{u,i}$.

$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$


```{r, echo = TRUE}
RMSE <- function(predicted_ratings, true_ratings){
  sqrt(mean((predicted_ratings - true_ratings)^2))
}
```

### 1. The Simplest Model: Average Movie Rating
We begin by building a very simple model: We predict a new movie rating to be the average rating of all
movies in our training dataset `edx`, completely disregarding the user who gives it. This gives us our baseline RMSE to compare future modelling approaches against. We observe that the mean movie rating $\mu$ is a pretty generous > 3.5 stars, quite a bit above the actual average (i.e. 2.5 stars).
  Our simplest model predicts $Y_{u,i}$ as $\mu$ plus the independent errors $\epsilon_{u,i}$, which are sampled from the same distribution and are centered at 0. This very simple model makes the assumption that all differences in movie ratings are explained by random variation alone.


$$ Y_{u, i} = \mu + \epsilon_{u, i}  $$

We create a table to record our approaches and the RMSEs they generate.


```{r, echo = FALSE}
mu <- mean(edx$rating)
baseline_RMSE <- RMSE(mu, validation$rating)
#baseline_RMSE


rmse_results <- data_frame(Method = "Average rating", RMSE = baseline_RMSE)
rmse_results %>% knitr::kable()
```

But can we do better than simply predicting the average rating? We will begin by incorporating some of the insights we gained during the exploratory data analysis.


### 2. Improving the Simplest Model: Incorporating the "Movie-effect"
To improve upon the model, we utilize the fact that movies are actually rated differently: Some movies are simply better or more popular than others, which is reflected in their rating being higher or lower than the average movie rating $\mu$.
  We compute the estimated deviation of each movies' mean rating from the total mean of all movies $\mu$.
We call the resulting variable "b" (as in "bias") for each movie "i": $\hat{b}_{i}$, which represents the average ranking of movie $i$. We add this term to our model equation from above.

Our model then becomes:

$$ Y_{u, i} = \mu + b_{i} + \epsilon_{u, i}  $$

```{r, echo = FALSE}
movie_avgs <- edx %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))
```


When plotting our computed $b_{i}$, we can see that there are large differences in how different movies are rated.


```{r, echo = FALSE, fig.height=4, fig.width=5}
movie_avgs %>% qplot(b_i, geom = "histogram", bins = 10, data = ., color = I("black"), fill = I("orange"))
```


We then predict movie ratings based on the fact that different movies are rated differently by adding the computed $b_{i}$ to $\mu$. If an individual movie is on average rated worse than the average rating of all movies $\mu$, we predict that it will be rated lower than $\mu$ by $b_{i}$, the difference of the individual movie average from the total average.


```{r, echo = FALSE}
predicted_ratings <- validation %>% 
  left_join(movie_avgs, by = "movieId") %>%
  mutate(pred = mu + b_i) %>%
  .$pred

model_1_RMSE <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results, data_frame(Method = "Movie Effect Model", RMSE = model_1_RMSE))
rmse_results %>% knitr::kable()
```

We can see that the RMSE improves when we take into account that different movies are rated differently. But this model is still in total disregard of individual user rating patterns. We will use some of the insights gained about user rating distribution to further improve upon our model.


### 3. Augmenting the Model with the "User-effect"
Do users rate different movies differently? We compute $b_{u}$, the "User-effect".
We can see that some users rate movies generally higher/lower than others, while most fall in-between, but also that user rating of movies is generally higher than a 2.5 "true average" rating. This was reflected previously in the high mean rating of > 3.5. We then predict the ratings taking into account movie and user effects together.

```{r, echo = FALSE, fig.height=3, fig.width=4}
edx %>% 
  group_by(userId) %>% 
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating)) %>% 
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black", fill = "orange") +
  xlab("Mean rating") +
  ylab("Number of users") +
  ggtitle("Mean movie ratings given by users") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  theme_light()
```

Our model then becomes:
$$ Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}  $$


And the corresponding RMSE value:
```{r, echo = FALSE}
user_avgs <- edx %>%
  left_join(movie_avgs, by = "movieId") %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- validation %>%
  left_join(movie_avgs, by = "movieId") %>%
  left_join(user_avgs, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

model_2_RMSE <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results, data_frame(Method = "Combined Movie & User Effects Model", 
                                                   RMSE = model_2_RMSE))
rmse_results %>% knitr::kable()
```


We can see that including the user-effect $b_{u}$ in our rating predictions further reduced the RMSE.
It appears that we are still off by ~0.865 stars on average. Are we correctly predicting the best and worst movies with our model? The top ten best and worst movies according to our predictions so far appear to include some "obscure" entries. This might be due to an overall low amount of ratings associated with these movies. We take a look at the amount of ratings given to them.
  We look at the top ten best movies according to our predictions so far (largest positive $b_{i}$), as well as at the number of ratings they received.
```{r, echo = FALSE}
movie_titles <- edx %>% 
  select(movieId, title) %>%
  distinct()

# Top 10 best movies according to our prediction with the largest positive b_i
# (movies rated better than average).
edx %>% count(movieId) %>% 
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(movie_titles, by = "movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```


The top ten worst movies according to our predictions so far (largest negative $b_{i}$).
```{r, echo = FALSE}
edx %>% count(movieId) %>% 
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(movie_titles, by = "movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

Indeed, some of the best and worst movies we predict were rated sparsely. Larger estimates of $b_{i}$ are likely for movies with very few ratings. The same holds true for the user-effect $b_{u}$, in those cases where users only rated a very small number of movies. We can penalize these by making use of regularization.
  We determine the value `Lambda` that minimizes RMSE, employing 3-way cross-validation. This shrinks the $b_{i}$ and $b_{u}$ in case of small number of ratings. Essentially, by shrinking our estimates when we are rather unsure, we are being more conservative in our estimations.

```{r, echo = FALSE, results = "hide"}
set.seed(1)
splitPlan <- kWayCrossValidation(nRows = nrow(edx), nSplits = 3, NULL, NULL) # We split our training data into k = 3 different train/test sets for cross-validation

lambdas <- seq(1.5, 3, 0.25) # We define the range of values we test for Lambda. The range has been set as small as possible to reduce computation times.
opt_lambda <- 0 # We initialize an empty vector that takes the results of the for-loop below

# NOTE: This code likely runs for several minutes, please be patient.
# The range for Lambda has been narrowed down after testing for all lambdas between 0 and 20 to shorten computation time here.

for (i in 1:length(splitPlan)){
  
split <- splitPlan[[i]]

rmses <- sapply(lambdas, function(lambda){
  
b_i <- edx[split$train, ] %>% 
  group_by(movieId) %>%
  summarize(b_i = sum(rating - mu)/(n() + lambda), n_i = n()) # Lambda is used to penalize b_i for small n()

# We generate a temporary test-set out of our training data edx
# by using the split from kWayCrossValidation. No information from our true validation set is used.
test_temp <- edx[split$app, ] %>%
  semi_join(edx[split$train, ], by = "movieId") %>%
  semi_join(edx[split$train, ], by = "userId")

# Prediction of ratings with temporary test-set
predicted_ratings <- test_temp %>%
  left_join(b_i, by = "movieId") %>%
  mutate(pred = mu + b_i) %>%
  .$pred

# Calculation of the resulting RMSE with Lambda
return(RMSE(predicted_ratings, test_temp$rating))
})

opt_lambda[i] <- lambdas[which.min(rmses)]

}
opt_lambda # All Lambda values from the k = 5 cross validations
b_i_opt_lambda <- mean(opt_lambda) # We use the mean as our optimal Lambda value
```


We then calculate the regularized movie-effect $b_{i}$ using the optimised `Lambda` value. We also apply the same approach to the user-effect $b_{u}$.


```{r, echo = FALSE}
movie_reg_avgs <- edx %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - mu)/(n() + b_i_opt_lambda), n_i = n()) 
```

```{r, echo = FALSE, results = "hide"}
###
# Regularization for the user effect b_u.
###


lambdas <- seq(4.5, 5.5, 0.25)
opt_lambda <- 0 # Empty vector that takes the output of the for-loop below


# NOTE: This code likely runs for several minutes, please be patient.
# The range for Lambda has been narrowed down after testing for all lambdas
# between 0 and 20 to shorten computation time here.
for (i in 1:length(splitPlan)){
  
split <- splitPlan[[i]]

rmses <- sapply(lambdas, function(lambda){
    
b_u <- edx[split$train, ] %>% 
      left_join(movie_reg_avgs, by = "movieId") %>%
      group_by(userId) %>%
      summarize(b_u = sum(rating - b_i - mu)/(n() + lambda), n_i = n())
    
test_temp <- edx[split$app, ] %>%
      semi_join(edx[split$train, ], by = "movieId") %>%
      semi_join(edx[split$train, ], by = "userId")
    
predicted_ratings <- test_temp %>%
      left_join(movie_reg_avgs, by = "movieId") %>%
      left_join(b_u, by = "userId") %>%
      mutate(pred = mu + b_i + b_u) %>%
      .$pred

# Calculation of the resulting RMSE with Lambda
return(RMSE(predicted_ratings, test_temp$rating))
})
  
opt_lambda[i] <- lambdas[which.min(rmses)]
  
}
opt_lambda
b_u_opt_lambda <- mean(opt_lambda)
```

```{r, echo = FALSE}
user_reg_avgs <- edx %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  group_by(userId) %>% 
  summarize(b_u = sum(rating - b_i - mu)/(n() + b_u_opt_lambda), n_i = n()) 
```

We then predict the ratings with the regularized movie- and user-effects.

```{r, echo = FALSE}
predicted_ratings <- validation %>%
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(user_reg_avgs, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

model_3_RMSE <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method = "Combined, regularized Movie & User Effect",
                                     RMSE = model_3_RMSE))
rmse_results %>% knitr::kable()
```

Regularization had little impact on the RMSE, but did it improve the best and worst movies we predict?
We take a look at the 10 best movies after regularization.

```{r, echo = FALSE}
edx %>%
  count(movieId) %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(movie_titles, by = "movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

And at the 10 worst movies after regularization.

```{r, echo = FALSE}
edx %>%
  count(movieId) %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(movie_titles, by = "movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

Indeed, the movies we predict to be the best/worst make much more sense now and are based on a larger number of ratings.
\pagebreak

# Results

## Accuracy of the Model

While the predicted ratings have the same mean as the original ratings, we observe values below 0.5 and above 5 due to the way we calculated them. Furthermore, we predicted numeric values and not categorical ratings from 0.5 to 5.
```{r, echo = TRUE}
summary(predicted_ratings)


head(predicted_ratings)
```


In order to predict star ratings from 0.5 to 5, we have to round our predictions and substitute values above 5 and below 0.5 accordingly. Here is the accuracy of our predictions in star ratings from 0.5 to 5.


```{r, echo = FALSE}
my_prediction <- predicted_ratings
my_prediction <- round(my_prediction/0.5)*0.5
my_prediction[my_prediction <= 0.5] <- 0.5 # Substitute all values below 0.5 with 0.5
my_prediction[my_prediction >= 5] <- 5 # Substitute all values above 5 with 5
```


```{r, echo = TRUE}
mean(my_prediction == validation$rating)
```



Here is our final RMSE value of the predicted ratings without rounding.

```{r, echo = FALSE}
RMSE(predicted_ratings, validation$rating)
```

```{r, echo = FALSE}
knitr::kable(rmse_results)
```

We arrive at an accuracy of 24.8% and an RMSE of 0.8648490 when modelling with the regularized movie and user effects.
\pagebreak

# Discussion






# Conclusion

The ~25% prediction accuracy value of this model can be explained by several facts. User preference for certain types of movies, genres, or even particular actors and directors most likely plays a major role in determining the ratings given. In fact, many modelling approaches in the Netflix challenge incorporated more user information than what was given here and it was found that user profession has a large predictive value. Other possible approaches would include item- and/or user-based collaborative filtering, where the relatively sparse matrix of user ratings (not every user rated every movie in the dataset, in fact far from it) is filled by matching similar user's preferences.
Whenever a user hates or loves just one or a few particular movies amongst all of his or her ratings, our model may completely miss the mark.