---
title: 'HarvardX: PH125.9x Data Science: Capstone - MovieLens Project'
author: "Martin Weihrauch"
date: "January 29, 2019"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---


# Executive Summary
This report begins by introducing the general idea and problem of the project and by stating its aims. After dataset preparation and setup, exploratory data analysis is carried out to gain the necessary insights into the data in order to develop a machine learning algorithm that predicts movie ratings. Next, the general modelling approach is broken down step-by-step and ends with the final model and its results in terms of prediction accuracy. This is then followed by a discussion section where strengths and weaknesses of the generated modelling approach are broken down and alternative approaches are briefly discussed. Finally, the report ends with some concluding remarks.

## Introduction
In 2006, Netflix offered a million US-Dollar price to whomever was able to surpass the performance of their movie recommendation algorithm by at least 10%. This goal was achieved by ["Team BellKor's Pragmatic Chaos""](https://www.netflixprize.com/community/topic_1537.html) in 2009, after a nearly 3-year long contest.
  Recommendation systems are a common occurrence nowadays and companies, big and small, employ them to increase sales numbers. It doesn't matter whether it is a new book on Amazon you might like to read, or the next popular song you might want to listen to on Spotify.
Movie recommendation systems work by knowing what a user likes (e.g. movie genres, actors) to then recommend more similar movies.
  It can become extremely challenging to recommend a movie to a user who really loves war dramas such as "Saving Private Ryan", but at the same time also really likes pacifist movies (or really hates one or a few particular war dramas, but loves all the rest). The more a recommendation system knows about what users like, the better a recommendation it can make.
  It is not the purpose of this project to give a "Top 10" best movies to watch recommendation list for a particular user, but instead to make predictions about the star rating users would give to different movies.


## Aim of the Project
The aim of this project is to build a machine learning algorithm that predicts user ratings (from 0.5 to 5 stars) for movies based on the dataset ["MovieLens"](http://files.grouplens.org/datasets/movielens/ml-10m.zip) with 10 million user ratings of over 10500 different movies.
  The algorithm is to be trained on a provided training subset (`edx`) of ["MovieLens"](http://files.grouplens.org/datasets/movielens/ml-10m.zip) and evaluation is to be carried out on the provided testing subset (`validation`).
  The metric used to evaluate algorithm performance is the residual mean squared error, or RMSE. This represents the typical error in star ratings the predicted rating would be off by.


## Dataset Preparation
The "MovieLens" dataset is automatically downloaded and prepared for analysis by splitting it into a training (`edx`, 90% of the data) and a test set (`validation`, 10% of the data). All necessary R packages are downloaded and loaded to execute the analysis. Algorithm development is to be carried out on the `edx` subset only, as `validation` will only be used to test the final algorithm. 

```{r, echo = FALSE, message = FALSE, warning = FALSE, eval = TRUE}
#############################################################
# Create edx set, validation set, and submission file
#############################################################

# Note: this process could take a couple of minutes
# The vtreat package will be used for cross-validation purposes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(vtreat)) install.packages("vtreat", repos = "http://cran.us.r-project.org")

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data

set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```
\pagebreak

# Methods and Analysis

## Exploratory Data Analysis
As a first step, we want to get familiar with the dataset. A quick glance at the first few entries reveals that each row represents a single rating of a user for a single movie, including the movie title, genre and timestamp at which the rating was given.


```{r, echo = FALSE}
head(edx) %>%
  print.data.frame()
```


A quick summary of the dataset tells us that there are no missing values to deal with (e.g. via a value imputation technique).



```{r, echo = FALSE}
summary(edx)
```


We are dealing with over 9 million movie ratings of ~70000 unique users giving ratings to ~ 10700 different movies.



```{r, echo = FALSE}
edx %>%
  summarize(n_users = n_distinct(userId), n_movies = n_distinct(movieId)) %>%
  print.data.frame()
```


We take a look at the distribution of ratings: 4 is the most common rating, followed by 3 and 5. 0.5 is the least common rating. It becomes apparent that half-star ratings are much less common than full-star ratings.



```{r, echo = FALSE}
edx %>%
  count(rating) %>%
  arrange(desc(n)) %>%
  print.data.frame()
```

To further strengthen our intuition about the rating distribution, we take a look at a histrogram of it.


```{r, echo = FALSE, fig.height=4, fig.width=5}
edx %>%
  ggplot(aes(rating)) +
  geom_histogram(binwidth = 0.5, color = "black", fill = "orange") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  scale_y_continuous(breaks = c(seq(0, 3000000, 500000))) +
  ggtitle("Rating distribution") +
  theme_light()
```


We take a look at the number of times different movies have been rated. We can observe that some movies have been rated much more often than others, while some have gotten very few and sometimes only a single rating. This will be important to consider for our modelling approach, as very low rating numbers might give untrustworthy estimates for predictions.


```{r echo = FALSE, fig.height=4, fig.width=5}
edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black", fill = "orange") + 
  scale_x_log10() + 
  xlab("Number of ratings [log10]") +
  ylab("Number of movies") +
  ggtitle("Number of ratings per movie") +
  theme_light()
```

What are some of these movies with very low ratings? We take a look at 20 movies that only received a single rating. As expected, these movies appear to be rather obscure, and predictions of future ratings for these will be difficult.

```{r echo = FALSE, fig.height=4, fig.width=5}
edx %>% 
  group_by(movieId) %>%
  summarize(count = n()) %>% 
  filter(count == 1) %>%
  left_join(edx, by = "movieId") %>%
  group_by(title) %>%
  summarize(rating = rating, n_rating = count) %>%
  slice(1:20) %>%
  knitr::kable()
```



Next we will explore user rating behaviour. Some users were very active and rated a large amount of movies, while others only rated a few.


```{r, echo = FALSE, fig.height=4, fig.width=5}
edx %>% count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black", fill = "orange") + 
  scale_x_log10() + 
  xlab("Number of ratings [log10]") +
  ylab("Number of users") +
  ggtitle("Number of ratings given by users") +
  theme_light()
```

Furthermore, users differ vastly in how critical they are with their ratings. Some users tend to give much lower star ratings and some users tend to give higher star ratings than average. The visualization below includes only users that have rated at least 100 movies.

```{r, echo = FALSE, fig.height=4, fig.width=5}
edx %>% 
  group_by(userId) %>% 
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating)) %>% 
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black", fill = "orange") +
  xlab("Mean rating") +
  ylab("Number of users") +
  ggtitle("Mean movie ratings given by users") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  theme_light()
```

We take a look at the representation of movie genres, their average ratings and the difference of their average rating from $\mu$ ("norm_avg_rating"). We can see that dramas and comedies are quite overrepresented. Some genres clearly get rated better or worse than others, for example Film-Noir (on average getting a 4-star rating), war movies, documentaries and IMAX movies are highly appraised, while horror movies are frowned upon. However, modelling these differences between genres is potentially difficult, as most movies tend to have more than just one genre associated with them and certain effects might cancel eachother out, or worse, mislead the prediction. While the Film-Noir genre has a mean rating of 4, there are still many movies getting as low as 0.5 stars as well.

```{r, echo = FALSE}
mu <- mean(edx$rating)

edx %>%
  separate_rows(genres, sep = "\\|") %>%
  group_by(genres) %>%
  summarize(count = n(), avg_rating = mean(rating), norm_avg_rating = mean(rating) - mu) %>%
  arrange(desc(count)) %>%
  print.data.frame()
```



\pagebreak

## Modelling Approach

We begin by writing a loss-function that computes the Residual Mean Squared Error (RMSE, or "typical error") as our measure of model accuracy. The value is the typical error in star rating we would make upon predicting a movie rating. $y_{u,i}$ here represents the rating for movie $i$ given by user $u$ and $N$ stands for the number of user/movie combinations. We denote our rating predictions from user $u$ for movie $i$ as $\hat{y}_{u,i}$.

$$ RMSE = \sqrt{\frac{1}{N}\displaystyle\sum_{u,i} (\hat{y}_{u,i}-y_{u,i})^{2}} $$


```{r, echo = TRUE}
RMSE <- function(predicted_ratings, true_ratings){
  sqrt(mean((predicted_ratings - true_ratings)^2))
}
```

### I. The Simplest Model: Average Movie Rating
After splitting `edx` into training and testing subsets, we begin by building a very simple model: We predict a new movie rating to be the average rating of all movies in our training dataset, completely disregarding the user who gives it. This gives us our baseline RMSE to compare future modelling approaches against. We observe that the mean movie rating $\mu$ is a pretty generous > 3.5 stars, quite a bit above the actual average (i.e. 2.5 stars).
  Our simplest model predicts $Y_{u,i}$ as $\mu$ plus the independent errors $\epsilon_{u,i}$, which are sampled from the same distribution and are centered at 0. This very simple model makes the assumption that all differences in movie ratings are explained by random variation alone.


$$ Y_{u, i} = \mu + \epsilon_{u, i}  $$

We create a table to record our approaches and the RMSEs they generate. We can see that by predicting a new rating to be the average rating we are typically off by over one star in rating (RMSE > 1.06)!


```{r, echo = FALSE}
# Before we begin building our model, we split our dataset edx into training and testing subsets.
set.seed(1)
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)
train_set <- edx[-test_index,]
test <- edx[test_index,]

# We make sure that userId and movieId in our test_set set are also in the train_set and we add the removed movieIds back into the train_set so we can predict against validation later.

test_set <- test %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")


removed <- test %>% 
  anti_join(train_set, by = "movieId")

train_set <- rbind(train_set, removed)

# Calculating mu and our baseline RMSE
mu <- mean(train_set$rating)
baseline_RMSE <- RMSE(mu, validation$rating)
#baseline_RMSE


rmse_results <- data_frame(Method = "Average rating", RMSE = baseline_RMSE)
rmse_results %>% knitr::kable()
```

But can we do better than simply predicting the average rating? We will begin by incorporating some of the insights we gained during the exploratory data analysis.


### II. Improving the Simplest Model by Incorporating the "Movie-effect"
To improve upon the model, we utilize the fact that movies are actually rated differently: Some movies are simply better or more popular than others, which is reflected in their rating being higher or lower than the average movie rating $\mu$.
  We compute the estimated deviation of each movies' mean rating from the total mean of all movies $\mu$.
We call the resulting variable "b" (as in "bias") for each movie "i": $\hat{b}_{i}$, which represents the average ranking of movie $i$. We add this term to our model equation from above.

Our model then becomes:

$$ Y_{u, i} = \mu + b_{i} + \epsilon_{u, i}  $$

```{r, echo = FALSE}
movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))
```


When plotting our computed $b_{i}$, we can see that there are large differences in how different movies are rated.


```{r, echo = FALSE, fig.height=4, fig.width=5}
movie_avgs %>% qplot(b_i, geom = "histogram", bins = 10, data = ., color = I("black"), fill = I("orange"))
```


We then predict movie ratings based on the fact that different movies are rated differently by adding the computed $b_{i}$ to $\mu$. If an individual movie is on average rated worse than the average rating of all movies $\mu$, we predict that it will be rated lower than $\mu$ by $b_{i}$, the difference of the individual movie average from the total average.


```{r, echo = FALSE}
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by = "movieId") %>%
  mutate(pred = mu + b_i) %>%
  .$pred

model_1_RMSE <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results, data_frame(Method = "Movie Effect", RMSE = model_1_RMSE))
rmse_results %>% knitr::kable()
```

We can see that the RMSE improves when we take into account that different movies are rated differently. But this model is still in total disregard of individual user rating patterns. We will use some of the insights gained about user rating distribution to further improve upon our model.


### III. Augmenting the Model with the "User-effect"
Do users rate different movies differently? We compute $b_{u}$, the "User-effect".
We can see that some users rate movies generally higher/lower than others, while most fall in-between, but also that user rating of movies is generally higher than a 2.5 "true average" rating. This was reflected previously in the high mean rating of > 3.5. We then predict the ratings taking into account movie and user effects together.

```{r, echo = FALSE, fig.height=3, fig.width=4}
train_set %>% 
  group_by(userId) %>% 
  filter(n() >= 100) %>%
  summarize(b_u = mean(rating)) %>% 
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black", fill = "orange") +
  xlab("Mean rating") +
  ylab("Number of users") +
  ggtitle("Mean movie ratings given by users") +
  scale_x_discrete(limits = c(seq(0.5,5,0.5))) +
  theme_light()
```

Our model then becomes:
$$ Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}  $$


And the corresponding RMSE value:
```{r, echo = FALSE}
user_avgs <- train_set %>%
  left_join(movie_avgs, by = "movieId") %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- test_set %>%
  left_join(movie_avgs, by = "movieId") %>%
  left_join(user_avgs, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

model_2_RMSE <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results, data_frame(Method = "Combined Movie & User Effects", 
                                                   RMSE = model_2_RMSE))
rmse_results %>% knitr::kable()
```


We can see that including the user-effect $b_{u}$ in our rating predictions further reduced the RMSE.
It appears that we are still off by ~0.865 stars on average. Are we correctly predicting the best and worst movies with our model? The top ten best and worst movies according to our predictions so far appear to include some "obscure" entries. This might be due to an overall low amount of ratings associated with these movies. We take a look at the amount of ratings given to them.
  We look at the top ten best movies according to our predictions so far (largest positive $b_{i}$), as well as at the number of ratings they received.
```{r, echo = FALSE}
movie_titles <- train_set %>% 
  select(movieId, title) %>%
  distinct()

# Top 10 best movies according to our prediction with the largest positive b_i
# (movies rated better than average).
train_set %>% count(movieId) %>% 
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(movie_titles, by = "movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```


The top ten worst movies according to our predictions so far (largest negative $b_{i}$).
```{r, echo = FALSE}
train_set %>% count(movieId) %>% 
  left_join(movie_avgs, by = 'movieId') %>%
  left_join(movie_titles, by = "movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

Indeed, some of the best and worst movies we predict were rated sparsely. Larger estimates of $b_{i}$ are likely for movies with very few ratings. The same holds true for the user-effect $b_{u}$, in those cases where users only rated a very small number of movies. We can penalize these by making use of regularization.
  We determine the value `Lambda` that minimizes RMSE, employing 3-way cross-validation. This shrinks the $b_{i}$ and $b_{u}$ in case of small number of ratings. Essentially, by shrinking our estimates when we are rather unsure, we are being more conservative in our estimations.

```{r, echo = FALSE, results = "hide"}
set.seed(1)
splitPlan <- kWayCrossValidation(nRows = nrow(train_set), nSplits = 3, NULL, NULL) # We split our training subset into k = 3 different train/test subsets for cross-validation.

lambdas <- seq(1.5, 3, 0.25) # We define the range of values we test for Lambda. The range has been set as small as possible to reduce computation times.
opt_lambda <- 0 # We initialize an empty vector that takes the results of the for-loop below

# NOTE: This code likely runs for several minutes, please be patient.
# The range for Lambda has been narrowed down after testing for all lambdas between 0 and 20 to shorten computation time here.

for (i in 1:length(splitPlan)){
  
  split <- splitPlan[[i]]
  
  rmses <- sapply(lambdas, function(lambda){
    
    b_i <- train_set[split$train, ] %>% 
      group_by(movieId) %>%
      summarize(b_i = sum(rating - mu)/(n() + lambda), n_i = n()) # Lambda is used to penalize b_i for small n()
    
    # We generate a temporary test-set out of our training data train_set
    # by using the split from kWayCrosstest_set. No information from our true test_set set is used.
    test_temp <- train_set[split$app, ] %>%
      semi_join(train_set[split$train, ], by = "movieId") %>%
      semi_join(train_set[split$train, ], by = "userId")
    
    # Prediction of ratings with temporary test-set
    predicted_ratings <- test_temp %>%
      left_join(b_i, by = "movieId") %>%
      mutate(pred = mu + b_i) %>%
      .$pred
    
    # Calculation of the resulting RMSE with Lambda
    return(RMSE(predicted_ratings, test_temp$rating))
  })
  
  opt_lambda[i] <- lambdas[which.min(rmses)]
  
}
opt_lambda # All Lambda values from the k = 3 cross test_sets
b_i_opt_lambda <- mean(opt_lambda) # We use the mean as our optimal Lambda value
```


We then calculate the regularized movie-effect $b_{i}$ using the optimised `Lambda` value. We also apply the same approach to the user-effect $b_{u}$.


```{r, echo = FALSE}
movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - mu)/(n() + b_i_opt_lambda), n_i = n()) 
```

```{r, echo = FALSE, results = "hide"}
###
# Regularization for the user effect b_u.
###


lambdas <- seq(4.5, 5.5, 0.25)
opt_lambda <- 0 # Empty vector that takes the output of the for-loop below


# NOTE: This code likely runs for several minutes, please be patient.
# The range for Lambda has been narrowed down after testing for all lambdas
# between 0 and 20 to shorten computation time here.
for (i in 1:length(splitPlan)){
  
  split <- splitPlan[[i]]
  
  rmses <- sapply(lambdas, function(lambda){
    
    b_u <- train_set[split$train, ] %>% 
      left_join(movie_reg_avgs, by = "movieId") %>%
      group_by(userId) %>%
      summarize(b_u = sum(rating - b_i - mu)/(n() + lambda), n_i = n())
    
    test_temp <- train_set[split$app, ] %>%
      semi_join(train_set[split$train, ], by = "movieId") %>%
      semi_join(train_set[split$train, ], by = "userId")
    
    predicted_ratings <- test_temp %>%
      left_join(movie_reg_avgs, by = "movieId") %>%
      left_join(b_u, by = "userId") %>%
      mutate(pred = mu + b_i + b_u) %>%
      .$pred
    
    # Calculation of the resulting RMSE with Lambda
    return(RMSE(predicted_ratings, test_temp$rating))
  })
  
  opt_lambda[i] <- lambdas[which.min(rmses)]
  
}
opt_lambda
b_u_opt_lambda <- mean(opt_lambda)
```

```{r, echo = FALSE}
user_reg_avgs <- train_set %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  group_by(userId) %>% 
  summarize(b_u = sum(rating - b_i - mu)/(n() + b_u_opt_lambda), n_i = n()) 
```

We then predict the ratings with the regularized movie- and user-effects.

```{r, echo = FALSE}
predicted_ratings <- test_set %>%
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(user_reg_avgs, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

model_3_RMSE <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method = "Combined, regularized Movie & User Effects",
                                     RMSE = model_3_RMSE))
rmse_results %>% knitr::kable()
```

Regularization had little impact on the RMSE, but did it improve the best and worst movies we predict?
We take a look at the 10 best movies after regularization.

```{r, echo = FALSE}
train_set %>%
  count(movieId) %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(movie_titles, by = "movieId") %>%
  arrange(desc(b_i)) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

And at the 10 worst movies after regularization.

```{r, echo = FALSE}
train_set %>%
  count(movieId) %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(movie_titles, by = "movieId") %>%
  arrange(b_i) %>% 
  select(title, b_i, n) %>% 
  slice(1:10) %>% 
  knitr::kable()
```

Indeed, the movies we predict to be the best/worst make much more sense now and are based on a larger number of ratings.
Before the final evaluation, we predict ratings on `validation` and calculate the RMSE of our algorithm on it. We find that our algorithm is doing a little bit worse on `validation` than it did when using it on the `test_set` we had generated from `edx`. 

```{r, echo = FALSE}
# Please note that the `validation` subset has not been used in any way to generate our algorithm.
predicted_ratings <- validation %>%
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(user_reg_avgs, by = "userId") %>%
  mutate(pred = mu + b_i + b_u) %>%
  .$pred

model_4_RMSE <- RMSE(predicted_ratings, validation$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(Method = "Combined regularized Movie & User Effect on `validation`",  
                                     RMSE = model_4_RMSE))
rmse_results %>% knitr::kable()
```


\pagebreak

# Results

While the predicted ratings have a similar mean compared to the original ratings, we observe values below 0.5 and above 5 due to the way we calculated them. Furthermore, we predicted numeric values and not categorical ratings from 0.5 to 5.
```{r, echo = TRUE}
summary(predicted_ratings)


head(predicted_ratings)
```


In order to predict star ratings from 0.5 to 5, we have to round our predictions and substitute values above 5 and below 0.5 accordingly. Here is the accuracy of our predictions in star ratings from 0.5 to 5.


```{r, echo = FALSE}
my_prediction <- predicted_ratings
my_prediction <- round(my_prediction/0.5)*0.5
my_prediction[my_prediction <= 0.5] <- 0.5 # Substitute all values below 0.5 with 0.5
my_prediction[my_prediction >= 5] <- 5 # Substitute all values above 5 with 5
```


```{r, echo = TRUE}
mean(my_prediction == validation$rating)
```



Here is our final RMSE value of the predicted ratings without rounding.

```{r, echo = FALSE}
RMSE(predicted_ratings, validation$rating) # Accuracy
```

```{r, echo = FALSE}
knitr::kable(rmse_results) # Final RMSE
```

We arrive at an accuracy of almost 24.8% and an RMSE of 0.8648490 when modelling with the regularized movie and user effects.
\pagebreak

# Discussion
Equation of the final model:

$$ Y_{u, i} = \mu + b_{i} + b_{u} + \epsilon_{u, i}  $$

The final model presented here performs reasonably well for the average user, that is when a user does not rate a particularly good/popular movie with a large positive $b_{i}$ unexpectedly (outside of his average rating given) low (dislikes a particular movie or genre/actor etc... ). The model is performing well when predicting a "critical" user (negative $b_{u}$) rating a good/popular movie (positive $b_{i}$). However, if a usually "critical" user (negative $b_{u}$) simply loves a particular average movie (neutral $b_{i}$), our prediction will be quite far off (the user might give a 5, but we predict a rating lower than $\mu$ because the movie is just average in terms of its $b_{i}$). Some users simply love every movie they rate, giving a full 5-star rating to every single one, resulting in a large $b_{u}$ in our predictions. Nevertheless, if this user rates below average movies (low or negative $b_{i}$), our prediction might not quite reach the full 5-star rating. Incorporating individual user preferences into the model would most likely lead to large improvements in prediction accuracy.

## Other Approaches
In the beginning the accuracy metric for this project was set on predicting the correct star ratings between 0.5 and 5 stars. Modelling the movie- and user-effects only returned about 25% accuracy. By making use of the fact that half-star ratings are much less common than full star ratings, one could round to full-star ratings only and forfeit prediction of any half-star ratings. This would bring up the prediction accuray to around 37%.
Incorporating genres and release year information in analogue fashion to user and movie effects yielded only negligible improvements of RMSE and star rating accuracy and were therefore removed from the modelling process of this report.
By utilizing the "recommenderlab" package, a user-based collaborative filtering model was built by generating a sparse rating matrix as input. However, once the built model was used to make predictions, RAM became a limiting factor and the predictions could not be finished.
By utilizing "ff", "ffbase", and "biglm" R-packages, it was attempted to run a linear model, but RAM issues persisted and these models could never be run successfully.
Principal component analysis (PCA) and singular value decomposition (SVD) were attempted, but failed due to RAM constraints.

As predicting star ratings from 0.5 to 5 can be seen as a classification rather than a regression problem, it was attempted to train different random forrest classifier algorithms on the dataset. However, once again the size of the dataset made running them almost impossible. Almost, because the "ranger" R-package actually successfully trained a random forrest with default parameters (running for many hours). The prediction accuracy was at around 33%, not really improving upon the model from above.
It was attempted to split the training dataset into many smaller parts, training different algorithms and combining results later, but no such approach yielded appreciable accuracy values.

An approach using naive bayes yielded approximately 34% accuracy when modelling with userId and movieId. Naive bayes assumes independence of the predictors, which in this scenario is not the case. In order to further improve prediction accuracy, it would have been necessary to incorporate user preferences into the equation. Due to any matrix factorization attempts failing, user preference were not successfully modeled and the approach was stopped.
\pagebreak

# Conclusion

For this capstone project, we built a regression model to predict movie ratings with the ["MovieLens"](http://files.grouplens.org/datasets/movielens/ml-10m.zip) dataset. The ~25% prediction accuracy value of the model presented here can be explained by several facts. User preference for certain types of movies, genres, or even particular actors and directors most likely plays a major role in determining the ratings given. In fact, many modelling approaches in the Netflix challenge incorporated more user information than what was given here and it was found that e.g. user profession has a large predictive value. Other possible approaches would include item- and/or user-based collaborative filtering, to incorporate individual user preferences into the model and thus most likely lead to a large increase in prediction accuracy.


# Acknowledgements

I would like to thank the entire edx course staff for providing a very stimulating learning environment throughout all of the courses and for making these learning materials available and exciting.
I thank all the active edx forum users for providing help and ample discussion.
And finally, a big thank you to my peers for spending their time and effort to review my capstone projects.
Thank you very much, everyone.